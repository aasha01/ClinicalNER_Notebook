{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b859b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35748ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Author: Geeticka Chauhan\n",
    "    Might be useful to also look at the code by https://github.com/yuanluo/seg_cnn\n",
    "    This file accompanies the notebook:\n",
    "    notebooks/Data-Preprocessing/i2b2-preprocessing/i2b2-processing-original.ipynb\n",
    "    Use the above notebook to create the original dataframe first, because the other \n",
    "    Notebooks in the i2b2-processing folder rely on the csv file generated in this notebook\n",
    "\"\"\"\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "\n",
    "relation_dict = {0: 'TrIP', 1: 'TrWP', 2: 'TrCP', 3: 'TrAP', 4: 'TrNAP', 5: 'TeRP', 6: 'TeCP', 7: 'PIP', 8:'None'}\n",
    "#relation_dict = {0: 'TrIP', 1: 'TrWP', 2: 'TrCP', 3: 'TrAP', 4: 'TrNAP', 5: 'TeRP', 6: 'TeCP', 7: 'PIP', 8: 'None'}\n",
    "rev_relation_dict = {'TrIP': 0, 'TrWP': 1, 'TrCP': 2, 'TrAP': 3, 'TrNAP': 4, 'TeRP': 5, 'TeCP': 6, 'PIP': 7, \n",
    "        'TrP-None': 8, 'TeP-None': 8, 'PP-None': 8}\n",
    "#rev_relation_dict = {val: key for key, val in relation_dict.items()}\n",
    "\n",
    "# given a file path, just get the name of the file\n",
    "def get_filename_with_extension(path):\n",
    "    return os.path.basename(path)\n",
    "\n",
    "# given the file name with an extension like filename.con, return the filename \n",
    "# without the extension i.e. filename\n",
    "def get_filename_without_extension(path):\n",
    "    filename_with_extension = os.path.basename(path)\n",
    "    return os.path.splitext(filename_with_extension)[0]\n",
    "\n",
    "# given a string that looks like c=\"concept\" extract the concept\n",
    "def extract_concept_from_string(fullstring):\n",
    "    return re.match(r'^c=\\\"(?P<concept>.*)\\\"$', fullstring).group('concept')\n",
    "\n",
    "# given a string that looks like t=\"type\" extract the type\n",
    "def extract_concept_type_from_string(fullstring):\n",
    "    return re.match(r'^t=\\\"(?P<type>.*)\\\"$', fullstring).group('type')\n",
    "\n",
    "# given a string that looks like r=\"TrAP\" extract the relation\n",
    "def extract_relation_from_string(fullstring):\n",
    "    return re.match(r'^r=\\\"(?P<relation>.*)\\\"$', fullstring).group('relation')\n",
    "\n",
    "# given a concept that looks like c=\"his home regimen\" 111:8 111:10, return the components\n",
    "def get_concept_subparts(concept):\n",
    "    concept_name = \" \".join(concept.split(' ')[:-2])\n",
    "    concept_name = extract_concept_from_string(concept_name)\n",
    "\n",
    "    concept_pos1 = concept.split(' ')[-2]\n",
    "    concept_pos2 = concept.split(' ')[-1]\n",
    "    return concept_name, concept_pos1, concept_pos2\n",
    "\n",
    "# given a position like 111:8 return the line number and word number\n",
    "def get_line_number_and_word_number(position):\n",
    "    split = position.split(':')\n",
    "    return split[0], split[1]\n",
    "\n",
    "# given a specific concept file path, generate a concept dictionary\n",
    "def get_concept_dictionary(file_path):\n",
    "    concept_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            concept = line.split('||')[0] # line splitting\n",
    "            type_of_concept = line.split('||')[1]\n",
    "            \n",
    "            type_of_concept = extract_concept_type_from_string(type_of_concept) # getting useful info\n",
    "            concept_name, concept_pos1, concept_pos2 = get_concept_subparts(concept)\n",
    "\n",
    "            line1, _ = get_line_number_and_word_number(concept_pos1)\n",
    "            line2, _ = get_line_number_and_word_number(concept_pos2)\n",
    "            if line1 != line2:\n",
    "                print(\"There is a problem! Concept spans multiple lines\")\n",
    "            \n",
    "            from_to_positions = concept_pos1 + \";\" + concept_pos2\n",
    "            concept_dict[from_to_positions] = {\n",
    "                    'fromto': from_to_positions, 'word': concept_name, 'type': type_of_concept}\n",
    "    return concept_dict\n",
    "\n",
    "# given a line number and the concept dictionary, return all the concepts from the \n",
    "# particular line #\n",
    "def get_entity_replacement_dictionary(linenum, concept_dict):\n",
    "    entity_replacement = {}\n",
    "    for key, val in concept_dict.items():\n",
    "        dict_linenum = key.split(';')[0].split(':')[0]\n",
    "        if dict_linenum == linenum:\n",
    "            fromword = key.split(';')[0].split(':')[1]\n",
    "            toword = key.split(';')[1].split(':')[1]\n",
    "            ent_repl_key = str(fromword) + ':' + str(toword)\n",
    "            entity_replacement[ent_repl_key] = val['type']\n",
    "    return entity_replacement # returns a list of dictionaries i.e. from-to, word, type\n",
    "\n",
    "# given a line in the relation file, return the concept1 word, spans, relation and concept 2 word, spans\n",
    "def read_rel_line(rel_line):\n",
    "    line = rel_line.strip()\n",
    "    concept1 = line.split('||')[0]\n",
    "    relation = line.split('||')[1]\n",
    "    concept2 = line.split('||')[2]\n",
    "\n",
    "    concept1_name, concept1_pos1, concept1_pos2 = get_concept_subparts(concept1)\n",
    "    concept2_name, concept2_pos1, concept2_pos2 = get_concept_subparts(concept2)\n",
    "    relation = extract_relation_from_string(relation)\n",
    "\n",
    "    line1_concept1, from_word_concept1 = get_line_number_and_word_number(concept1_pos1)\n",
    "    line2_concept1, to_word_concept1  = get_line_number_and_word_number(concept1_pos2)\n",
    "\n",
    "    line1_concept2, from_word_concept2 = get_line_number_and_word_number(concept2_pos1)\n",
    "    line2_concept2, to_word_concept2 = get_line_number_and_word_number(concept2_pos2)\n",
    "\n",
    "    if line1_concept1 != line2_concept1 or line1_concept2 != line2_concept2 or \\\n",
    "            line1_concept1 != line1_concept2:\n",
    "                print(\"Concepts are in two different lines\")\n",
    "    # assuming that all the lines are the same\n",
    "    return {'e1_word': concept1_name, 'e1_from': from_word_concept1, 'e1_to': to_word_concept1,\n",
    "            'e2_word': concept2_name, 'e2_from': from_word_concept2, 'e2_to': to_word_concept2, \n",
    "            'line_num': line1_concept1, 'relation': relation}\n",
    "\n",
    "# below is for the case that you do not want to extract the None relations from the data, \n",
    "# because that is inferred from the concept types rather than explicitly present in the \n",
    "# relation annotations\n",
    "# give it a directory with res(directory + 'concept/')\n",
    "def get_dataset_dataframe_classification(concept_directory, rel_directory, txt_directory):\n",
    "    data = []\n",
    "    total_rel_files_to_read = glob.glob(os.path.join(rel_directory, '*'))\n",
    "    \n",
    "    for rel_file_path in tqdm(total_rel_files_to_read):\n",
    "        with open(rel_file_path, 'r') as rel_file:\n",
    "            base_filename = get_filename_without_extension(rel_file_path)\n",
    "            concept_file_path = os.path.join(concept_directory, base_filename +\".con\")\n",
    "            concept_dictionary = get_concept_dictionary(concept_file_path)\n",
    "            \n",
    "            text_file_path = os.path.join(txt_directory, base_filename +\".txt\")\n",
    "            text_file = open(text_file_path, 'r').readlines() \n",
    "\n",
    "            for rel_line in rel_file:\n",
    "                rel_dict = read_rel_line(rel_line)\n",
    "                tokenized_sentence = text_file[int(rel_dict['line_num']) - 1].strip()\n",
    "                sentence_text = tokenized_sentence\n",
    "                e1 = rel_dict['e1_word']\n",
    "                e2 = rel_dict['e2_word']\n",
    "                relation_type = rel_dict['relation']\n",
    "                linenum = rel_dict['line_num']\n",
    "                entity_replacement_dict = get_entity_replacement_dictionary(linenum, concept_dictionary)\n",
    "                \n",
    "                e1_idx = [(rel_dict['e1_from'], rel_dict['e1_to'])]\n",
    "                e2_idx = [(rel_dict['e2_from'], rel_dict['e2_to'])]\n",
    "\n",
    "                metadata = {'e1': {'word': str(e1), 'word_index': e1_idx},\n",
    "                            'e2': {'word': str(e2), 'word_index': e2_idx},\n",
    "                            'entity_replacement': entity_replacement_dict,\n",
    "                            'sentence_id': str(linenum), # numbering starts from 1\n",
    "                            'filename': str(base_filename)}\n",
    "                data.append([str(sentence_text), str(e1), str(e2), str(relation_type), metadata,\n",
    "                    str(tokenized_sentence)])\n",
    "\n",
    "    df = pd.DataFrame(data,\n",
    "            columns='original_sentence,e1,e2,relation_type,metadata,tokenized_sentence'.split(','))\n",
    "    return df\n",
    "\n",
    "'''\n",
    "Get the dataframe for the extraction case\n",
    "'''\n",
    "# arrange the concepts in the concept dict by the linenumber for faster processing\n",
    "def get_concepts_by_linenum(concept_dict):\n",
    "    concept_dict_by_linenum = {}\n",
    "    for key in concept_dict.keys():\n",
    "        linenum, _ = get_line_number_and_word_number(key)\n",
    "        linenum = str(linenum)\n",
    "        if linenum in concept_dict_by_linenum:\n",
    "             concept_dict_by_linenum[linenum].append(key)\n",
    "        else:\n",
    "            concept_dict_by_linenum[linenum] = [key]\n",
    "    return concept_dict_by_linenum\n",
    "\n",
    "# arrange the relation pairs in a list by their linenumber for faster processing\n",
    "def get_relation_pair_by_linenum(artificial_relations_pair):\n",
    "    non_existant_relations_by_linenum = {}\n",
    "    for relation_pair in artificial_relations_pair:\n",
    "        relation_pair = list(relation_pair)\n",
    "        linenum, _ = get_line_number_and_word_number(relation_pair[0])\n",
    "        linenum2, _ = get_line_number_and_word_number(relation_pair[1])\n",
    "        if linenum != linenum2: print('Problem! The relation pair ', relation_pair, ' spans multiple lines.')\n",
    "        linenum = str(linenum)\n",
    "        if linenum in non_existant_relations_by_linenum:\n",
    "            non_existant_relations_by_linenum[linenum].append(relation_pair)\n",
    "        else:\n",
    "            non_existant_relations_by_linenum[linenum] = [relation_pair]\n",
    "    return non_existant_relations_by_linenum\n",
    "\n",
    "\n",
    "# check if the relation pair already exists in the artificial relations pair generated\n",
    "# written in the form of linenum:from;linenum:to as keys to the concept dictionary\n",
    "def relation_exist_in_pair_list(artificial_relations_pair, relation_pair):\n",
    "    for concept_pair in artificial_relations_pair:\n",
    "        if relation_pair == concept_pair:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# generate a list of 'artificial relation pairs' - according to i2b2 dataset providers\n",
    "# relations can exist between the Problem-Problem, Problem-Treatment and Problem-Test \n",
    "# relations. The job of this function is to provide an initial list from that.\n",
    "def get_artificial_relation_pair(concept_dict_by_linenum, concept_dict):\n",
    "    artificial_relations_pair = []\n",
    "    for linenum in concept_dict_by_linenum.keys():\n",
    "        concepts = concept_dict_by_linenum[linenum]\n",
    "        for i1, concept1 in enumerate(concepts):\n",
    "            indexes_to_look_through = list(range(0, i1))\n",
    "            indexes_to_look_through.extend(range(i1+1, len(concepts)))\n",
    "            for i2 in indexes_to_look_through:\n",
    "                concept2 = concepts[i2]\n",
    "                relation_pair = {concept1, concept2}\n",
    "                if relation_exist_in_pair_list(artificial_relations_pair, relation_pair):\n",
    "                    continue\n",
    "                # if it doesn't already exist in the artificial relation pair, the following runs\n",
    "                type_pair = {concept_dict[concept1]['type'], concept_dict[concept2]['type']}\n",
    "                if type_pair == {'problem', 'problem'} or type_pair == {'problem', 'test'} or \\\n",
    "                type_pair == {'problem', 'treatment'}:\n",
    "                    artificial_relations_pair.append(relation_pair)\n",
    "    return artificial_relations_pair\n",
    "\n",
    "# append existing relations to a list known as data, to populate the dataframe later\n",
    "def append_existing_relations(data, rel_file, text_file, base_filename, artificial_relations_pair, \n",
    "        concept_dict):\n",
    "    for rel_line in rel_file:\n",
    "        rel_dict = read_rel_line(rel_line)\n",
    "        tokenized_sentence = text_file[int(rel_dict['line_num']) - 1].strip()\n",
    "        sentence_text = tokenized_sentence\n",
    "        e1 = rel_dict['e1_word']\n",
    "        e2 = rel_dict['e2_word']\n",
    "        relation_type = rel_dict['relation']\n",
    "        linenum = rel_dict['line_num']\n",
    "        entity_replacement_dict = get_entity_replacement_dictionary(linenum, concept_dict)\n",
    "\n",
    "        concept1_key = linenum + ':' + rel_dict['e1_from'] + ';' + linenum + ':' + rel_dict['e1_to']\n",
    "        concept2_key = linenum + ':' + rel_dict['e2_from'] + ';' + linenum + ':' + rel_dict['e2_to']\n",
    "        relation_pair = {concept1_key, concept2_key}\n",
    "        # There is a bug in the data - annotations where they have a pair which is treatment-treatment\n",
    "        # In such cases just print those cases that are not present in the list and will send an error\n",
    "        # catch the error and print something\n",
    "        try:\n",
    "            artificial_relations_pair.remove(relation_pair)\n",
    "        except ValueError: \n",
    "            print('Message from append_existing_relations(): The relation pair ', relation_pair, \n",
    "                    'is not present in the artificial relations pair and their respective types are ', \n",
    "                    concept_dict[concept1_key]['type'], concept_dict[concept2_key]['type'])\n",
    "        # this should not have errored out - maybe there is an error in the artificial \n",
    "\n",
    "        e1_idx = [(rel_dict['e1_from'], rel_dict['e1_to'])]\n",
    "        e2_idx = [(rel_dict['e2_from'], rel_dict['e2_to'])]\n",
    "\n",
    "        metadata = {'e1': {'word': str(e1), 'word_index': e1_idx},\n",
    "                    'e2': {'word': str(e2), 'word_index': e2_idx},\n",
    "                    'entity_replacement': entity_replacement_dict,\n",
    "                    'sentence_id': str(linenum), # numbering starts from 1\n",
    "                    'filename': str(base_filename)}\n",
    "        data.append([str(sentence_text), str(e1), str(e2), str(relation_type), metadata,\n",
    "            str(tokenized_sentence)])\n",
    "    return data, artificial_relations_pair\n",
    "\n",
    "# given concept dictionary and relation pair, assign the right ordering to entity1 and entity2\n",
    "# Treatment appears first, then problem and similar with Test-Problem\n",
    "# But for Problem-Problem, they are arranged by whichever appears first in the sentence\n",
    "def assign_e1_e2_relation(concept_dict, relation_pair):\n",
    "    type1 = concept_dict[relation_pair[0]]['type']\n",
    "    type2 = concept_dict[relation_pair[1]]['type']\n",
    "    if type1 == 'problem' and type2 == 'problem':\n",
    "        # in this case we will arrange by whichever appears first in the sentence\n",
    "        _, wordnum1 = get_line_number_and_word_number(relation_pair[0].split(';')[0]) # just judging by the from\n",
    "        _, wordnum2 = get_line_number_and_word_number(relation_pair[1].split(';')[0])\n",
    "        if int(wordnum1) < int(wordnum2):\n",
    "            entity1 = relation_pair[0]\n",
    "            entity2 = relation_pair[1]\n",
    "        elif int(wordnum1) > int(wordnum2):\n",
    "            entity1 = relation_pair[1]\n",
    "            entity2 = relation_pair[0]\n",
    "        else: print('There is a problem! Two entities are starting at the same number. Unexpected. ')\n",
    "        relation_type = 'PP-None'\n",
    "    elif type1 == 'treatment' and type2 == 'problem':\n",
    "        entity1 = relation_pair[0]\n",
    "        entity2 = relation_pair[1]\n",
    "        relation_type = 'TrP-None'\n",
    "    elif type1 == 'problem' and type2 == 'treatment':\n",
    "        entity1 = relation_pair[1]\n",
    "        entity2 = relation_pair[0]\n",
    "        relation_type = 'TrP-None'\n",
    "    elif type1 == 'test' and type2 == 'problem':\n",
    "        entity1 = relation_pair[0]\n",
    "        entity2 = relation_pair[1]\n",
    "        relation_type = 'TeP-None'\n",
    "    elif type1 == 'problem' and type2 == 'test':\n",
    "        entity1 = relation_pair[1]\n",
    "        entity2 = relation_pair[0]\n",
    "        relation_type = 'TeP-None'\n",
    "    else: print('Message from assign_e1_e2_relation(): This pairing of the types should not be possible!')\n",
    "    return entity1, entity2, relation_type\n",
    "    \n",
    "\n",
    "# append the relations that do not exist to a list known as data \n",
    "def append_non_existing_relations(data, text_file, base_filename, relation_pair_by_linenum, concept_dict):\n",
    "    for linenum in relation_pair_by_linenum.keys():\n",
    "        relation_pairs = relation_pair_by_linenum[linenum]\n",
    "        entity_replacement_dict = get_entity_replacement_dictionary(linenum, concept_dict)\n",
    "        # the idea is that going line by line rather than per relation pair, we are \n",
    "        # saving computation time on the above\n",
    "        tokenized_sentence = text_file[int(linenum) - 1].strip()\n",
    "        for relation_pair in relation_pairs:\n",
    "            sentence_text = tokenized_sentence\n",
    "            # e1 and e2 are decided based on the type\n",
    "                # in problem-problem cases, we arange based on which word appears first in the sentence\n",
    "                # in the problem-treatment cases, treatment always goes first\n",
    "                # in the problem-test cases, test always goes first\n",
    "            entity1, entity2, relation_type = assign_e1_e2_relation(concept_dict, relation_pair)\n",
    "            e1 = concept_dict[entity1]['word']\n",
    "            e2 = concept_dict[entity2]['word']\n",
    "\n",
    "            entity1_from_linenum, entity1_to_linenum = entity1.split(';')\n",
    "            entity2_from_linenum, entity2_to_linenum = entity2.split(';')\n",
    "\n",
    "            _, entity1_from = get_line_number_and_word_number(entity1_from_linenum)\n",
    "            _, entity1_to = get_line_number_and_word_number(entity1_to_linenum)\n",
    "            _, entity2_from = get_line_number_and_word_number(entity2_from_linenum)\n",
    "            _, entity2_to = get_line_number_and_word_number(entity2_to_linenum)\n",
    "\n",
    "            e1_idx = [(entity1_from, entity1_to)]\n",
    "            e2_idx = [(entity2_from, entity2_to)]\n",
    "\n",
    "            metadata = {'e1': {'word': str(e1), 'word_index': e1_idx},\n",
    "                        'e2': {'word': str(e2), 'word_index': e2_idx},\n",
    "                        'entity_replacement': entity_replacement_dict,\n",
    "                        'sentence_id': str(linenum),\n",
    "                        'filename': str(base_filename)}\n",
    "            data.append([str(sentence_text), str(e1), str(e2), str(relation_type), metadata, \n",
    "                        str(tokenized_sentence)])\n",
    "    return data\n",
    "\n",
    "# populate the dataframe for the extraction case when we also train and test on non existing \n",
    "# relation pairs\n",
    "def get_dataset_dataframe_extraction(concept_directory, rel_directory, txt_directory):\n",
    "    data = []\n",
    "    total_rel_files_to_read = glob.glob(os.path.join(rel_directory, '*'))\n",
    "    \n",
    "    for rel_file_path in tqdm(total_rel_files_to_read):\n",
    "        with open(rel_file_path, 'r') as rel_file:\n",
    "            base_filename = get_filename_without_extension(rel_file_path)\n",
    "            concept_file_path = os.path.join(concept_directory, base_filename + '.con')\n",
    "            concept_dictionary = get_concept_dictionary(concept_file_path)\n",
    "            \n",
    "            text_file_path = os.path.join(txt_directory, base_filename + '.txt')\n",
    "            text_file = open(text_file_path, 'r').readlines()\n",
    "            concept_dict_by_linenum = get_concepts_by_linenum(concept_dictionary)\n",
    "            \n",
    "            # these are all the viable relation pairs that could exist in the list\n",
    "            artificial_relations_pair = get_artificial_relation_pair(concept_dict_by_linenum, concept_dictionary)\n",
    "            \n",
    "            data, artificial_relations_pair = append_existing_relations(data, rel_file, text_file, base_filename,\n",
    "                                                                        artificial_relations_pair, \n",
    "                                                                        concept_dictionary)\n",
    "            relation_pair_by_linenum = get_relation_pair_by_linenum(artificial_relations_pair)\n",
    "            \n",
    "            data = append_non_existing_relations(data, text_file, base_filename, relation_pair_by_linenum,\n",
    "                                                concept_dictionary)\n",
    "    df = pd.DataFrame(data,\n",
    "            columns='original_sentence,e1,e2,relation_type,metadata,tokenized_sentence'.split(','))\n",
    "    return df\n",
    "\n",
    "def get_dataset_dataframe(concept_directory, rel_directory, txt_directory, extract_none_relations=True):\n",
    "    if extract_none_relations is True:\n",
    "        function_to_call = get_dataset_dataframe_extraction\n",
    "    else:\n",
    "        function_to_call = get_dataset_dataframe_classification\n",
    "    return function_to_call(concept_directory, rel_directory, txt_directory)\n",
    "\n",
    "\n",
    "# to streamline the writing of the dataframe\n",
    "def write_dataframe(df, directory):\n",
    "    df.to_csv(directory, sep='\\t', encoding='utf-8', index=False)\n",
    "\n",
    "# to streamline the reading of the dataframe\n",
    "def read_dataframe(directory):\n",
    "    df = pd.read_csv(directory, sep='\\t')\n",
    "    def literal_eval_metadata(row):\n",
    "        metadata = row.metadata\n",
    "        metadata = literal_eval(metadata)\n",
    "        return metadata\n",
    "    df['metadata'] = df.apply(literal_eval_metadata, axis=1)\n",
    "    # metadata is a dictionary which is written into the csv format as a string\n",
    "    # but in order to be treated as a dictionary it needs to be evaluated\n",
    "    return df\n",
    "\n",
    "# The goal here is to make sure that the df that is written into memory is the same one that is read\n",
    "def check_equality_of_written_and_read_df(df, df_copy):\n",
    "    bool_equality = df.equals(df_copy)\n",
    "    # to double check, we want to check with every column\n",
    "    bool_every_column = True\n",
    "    for idx in range(len(df)):\n",
    "        row1 = df.iloc[idx]\n",
    "        row2 = df_copy.iloc[idx]\n",
    "        if row1['original_sentence'] != row2['original_sentence'] or row1['e1'] != row2['e1'] or \\\n",
    "                row1['relation_type'] != row2['relation_type'] or \\\n",
    "                row1['tokenized_sentence'] != row2['tokenized_sentence'] or \\\n",
    "                row1['metadata'] != row2['metadata']: \n",
    "                    bool_every_column = False\n",
    "                    break\n",
    "    return bool_equality, bool_every_column\n",
    "\n",
    "\n",
    "# write the dataframe into the text format accepted by the cnn model\n",
    "def write_into_txt(df, directory):\n",
    "    print(\"Unique relations: \\t\", df['relation_type'].unique())\n",
    "    null_row = df[df[\"relation_type\"].isnull()]\n",
    "    if null_row.empty:\n",
    "        idx_null_row = None\n",
    "    else:\n",
    "        idx_null_row = null_row.index.values[0]\n",
    "    with open(directory, 'w') as outfile:\n",
    "        for i in range(0, len(df)):\n",
    "            if idx_null_row is not None and i == idx_null_row:\n",
    "                continue\n",
    "            row = df.iloc[i]\n",
    "            relation = rev_relation_dict[row.relation_type]\n",
    "            metadata = row.metadata\n",
    "# TODO: need to change below in order to contain a sorted list of the positions\n",
    "# metadata['e1']['word_index'] returns a list of tuples\n",
    "            e1 = metadata['e1']['word_index'][0]\n",
    "            e2 = metadata['e2']['word_index'][0]\n",
    "            tokenized_sentence = row.tokenized_sentence\n",
    "            outfile.write(str(relation) + \" \" + str(e1[0]) + \" \" + str(e1[-1]) + \" \" + \n",
    "                          str(e2[0]) + \" \" + str(e2[-1]) + \" \" + tokenized_sentence + \"\\n\")\n",
    "        outfile.close()\n",
    "\n",
    "\n",
    "# combine txt files of beth and partners\n",
    "def combine(res, outdir, file1, file2, outfilename):\n",
    "    outfile = outdir + outfilename\n",
    "    # https://stackoverflow.com/questions/13613336/python-concatenate-text-files\n",
    "    filenames = [res(outdir + file1+'.txt'), res(outdir + file2+'.txt')]\n",
    "    with open(res(outfile), 'w') as outfile:\n",
    "        for fname in filenames:\n",
    "            with open(fname) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9660a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF210GPUJupyter",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
